{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emissions Intensity Scheme (EIS) Parameter Selection\n",
    "Process data from DCOPF and MPPDC models.\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare paths to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier used to update paths depending on the number of scenarios investigated\n",
    "number_of_scenarios = '100_scenarios'\n",
    "\n",
    "# Core data directory\n",
    "data_dir = os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, 'data')\n",
    "\n",
    "# Operating scenario data\n",
    "operating_scenarios_dir = os.path.join(os.path.curdir, os.path.pardir, '1_create_scenarios')\n",
    "\n",
    "# Model output directory\n",
    "parameter_selector_dir = os.path.join(os.path.curdir, os.path.pardir, '2_parameter_selector', 'output', number_of_scenarios)\n",
    "\n",
    "# Output directory\n",
    "output_dir = os.path.join(os.path.curdir, 'output', number_of_scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: SRMCs were adjusted for generators and node reindexed in the parameter_selector notebook.\n",
    "# Therefore data should be loaded from the parameter_selector output folder\n",
    "\n",
    "# Generator data\n",
    "with open(os.path.join(parameter_selector_dir, 'df_g.pickle'), 'rb') as f:\n",
    "    df_g = pickle.load(f)\n",
    "    \n",
    "# Node data\n",
    "with open(os.path.join(parameter_selector_dir, 'df_n.pickle'), 'rb') as f:\n",
    "    df_n = pickle.load(f)\n",
    "    \n",
    "# Scenario data\n",
    "with open(os.path.join(parameter_selector_dir, 'df_scenarios.pickle'), 'rb') as f:\n",
    "    df_scenarios = pickle.load(f)\n",
    "    \n",
    "# DCOPF results for BAU scenario - (baseline=0, permit price=0)\n",
    "with open(os.path.join(parameter_selector_dir, 'DCOPF-FIXED_PARAMETERS-PERMIT_PRICE_0-BASELINE_0.pickle'), 'rb') as f:\n",
    "    df_dcopf = pickle.load(f)\n",
    "    \n",
    "# MPPDC results for BAU scenario - (baseline=0, permit price=0)\n",
    "with open(os.path.join(parameter_selector_dir, 'MPPDC-FIXED_PARAMETERS-BASELINE_0-PERMIT_PRICE_0.pickle'), 'rb') as g:\n",
    "    df_mppdc = pickle.load(g) \n",
    "\n",
    "# Function used to collate DataFrames for different modelling scenarios\n",
    "def collate_data(filename_contains):\n",
    "    \"\"\"Collate data for different scenarios into a single DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename_contains : str\n",
    "        Partial filename used to filter files in parameter_selector output directory\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_o : pandas DataFrame\n",
    "        Collated results for specified model type    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtered files\n",
    "    files = [os.path.join(parameter_selector_dir, f) for f in os.listdir(os.path.join(parameter_selector_dir)) if filename_contains in f]\n",
    "    \n",
    "    # Container for inidividual scenarios\n",
    "    dfs = []\n",
    "    \n",
    "    # Loop through files and load results objects\n",
    "    for f in files:\n",
    "        # Load results\n",
    "        with open(f, 'rb') as g:\n",
    "            df = pickle.load(g)\n",
    "\n",
    "            # Filter column names\n",
    "            filtered_columns = [i for i in df.columns if i not in ['Gap', 'Status', 'Message', 'Problem', 'Objective', 'Constraint']]\n",
    "\n",
    "            # Filter DataFrame\n",
    "            df_filtered = df.loc[df.index.str.contains(r'\\.P\\[|\\.H\\[|\\.vang\\[|\\.lambda_var\\[|phi'), filtered_columns]\n",
    "\n",
    "            # Append to container which will later be concatenated\n",
    "            dfs.append(df_filtered)\n",
    "    \n",
    "    # Concatenate results\n",
    "    df_o = pd.concat(dfs)\n",
    "    \n",
    "    return df_o    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare DCOPF and MPPDC models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare primal variables between DCOPF and MPPDC models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable_index_1\n",
       "H       4.200000e+02\n",
       "P       3.527604e-07\n",
       "vang    1.534596e+00\n",
       "Name: Value, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_primal_variables(df_dcopf, df_mppdc):\n",
    "    \"\"\"Verify that MPPDC and DCOPF results match\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_dcopf : pandas DataFrame\n",
    "        DataFrame containing results for DCOPF model\n",
    "    \n",
    "    df_mppdc : pandas DataFrame\n",
    "        DataFrame containing results for MPPDC model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_max_difference : pandas DataFrame\n",
    "        Max difference across all scenarios for each primal variable    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Process DCOPF DataFrame\n",
    "    # -----------------------\n",
    "    df_dcopf = df_dcopf.reset_index()\n",
    "    df_dcopf = df_dcopf[df_dcopf['index'].str.contains(r'\\.P\\[|\\.H\\[|\\.vang\\[')]\n",
    "    \n",
    "    # Extract values for primal variables\n",
    "    df_dcopf['Value'] = df_dcopf.apply(lambda x: x['Variable']['Value'], axis=1)\n",
    "\n",
    "    # Extract scenario ID\n",
    "    df_dcopf['SCENARIO_ID'] = df_dcopf['SCENARIO_ID'].astype(int)\n",
    "    \n",
    "    # Extract variable names and indices\n",
    "    df_dcopf['variable_index_1'] = df_dcopf['index'].str.extract(r'\\.(.+)\\[')\n",
    "    df_dcopf['variable_index_2'] = df_dcopf['index'].str.extract(r'\\.[A-Za-z]+\\[(.+)\\]$')\n",
    "    \n",
    "    # Reset index\n",
    "    df_dcopf = df_dcopf.set_index(['SCENARIO_ID', 'variable_index_1', 'variable_index_2'])['Value']\n",
    "    \n",
    "    \n",
    "    # Process MPPDC DataFrame\n",
    "    # -----------------------\n",
    "    df_mppdc = df_mppdc.reset_index()\n",
    "    \n",
    "    # Extract values for primal variables\n",
    "    df_mppdc = df_mppdc[df_mppdc['index'].str.contains(r'\\.P\\[|\\.H\\[|\\.vang\\[')]\n",
    "    df_mppdc['Value'] = df_mppdc.apply(lambda x: x['Variable']['Value'], axis=1)\n",
    "\n",
    "    # Extract scenario ID\n",
    "    df_mppdc['SCENARIO_ID'] = df_mppdc['index'].str.extract(r'\\[(\\d+)\\]\\.')\n",
    "    df_mppdc['SCENARIO_ID'] = df_mppdc['SCENARIO_ID'].astype(int)\n",
    "\n",
    "    # Extract variable names and indices\n",
    "    df_mppdc['variable_index_1'] = df_mppdc['index'].str.extract(r'\\.(.+)\\[')\n",
    "    df_mppdc['variable_index_2'] = df_mppdc['index'].str.extract(r'\\.[A-Za-z]+\\[(.+)\\]$')\n",
    "    \n",
    "    # Reset index\n",
    "    df_mppdc = df_mppdc.set_index(['SCENARIO_ID', 'variable_index_1', 'variable_index_2'])['Value']\n",
    "    \n",
    "    # Max difference across all time intervals and primal variables\n",
    "    df_max_difference = (df_dcopf.subtract(df_mppdc)\n",
    "                         .reset_index()\n",
    "                         .groupby(['variable_index_1'])['Value']\n",
    "                         .apply(lambda x: x.abs().max()))\n",
    "    \n",
    "    return df_max_difference\n",
    "\n",
    "# Find max difference in primal variables between DCOPF and MPPDC models over all scenarios\n",
    "compare_primal_variables(df_dcopf, df_mppdc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that voltage angles and HVDC flows do not correspond exactly, but power output and prices do. This is likely due to the introduction of an additional degree of freedom when adding HVDC links into the network analysis. Having HVDC links allows power to flow over either the HVDC or AC networks. So long as branch flows are within limits for constrained links, different combinations of these flows may be possible. This results in different intra-zonal flows (hence different voltage angles), but net inter-zonal flows are the same as the DCOPF case.\n",
    "\n",
    "These differences are likely due to the way in which the solver approaches a solution; different feasible HVDC and intra-zonal AC flows yield the same least-cost dispatch. Consequently, DCOPF and MPPDC output corresponds, but HVDC flows and node voltage angles (which relate to AC power flows) do not. As an additional check average and nodal prices are compared between the DCOPF and MPPDC models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute difference between DCOPF and MPPDC average prices: 3.377209623067756e-11 [$/MWh]\n"
     ]
    }
   ],
   "source": [
    "def get_dcopf_average_price(df, variable_name_contains):\n",
    "    \"Find average price for DCOPF BAU scenario\"\n",
    "    \n",
    "    df_tmp = df.reset_index().copy()\n",
    "\n",
    "    # Filter price records\n",
    "    df_tmp = df_tmp[df_tmp['index'].str.contains(r'\\.{0}\\['.format(variable_name_contains))]\n",
    "    \n",
    "    # Extract values\n",
    "    df_tmp['Value'] = df_tmp.apply(lambda x: x['Constraint']['Dual'], axis=1)\n",
    "\n",
    "    # Extract node and scenario IDs\n",
    "    df_tmp['NODE_ID'] = df_tmp['index'].str.extract(r'\\.{0}\\[(\\d+)\\]'.format(variable_name_contains)).astype(int)\n",
    "    df_tmp['SCENARIO_ID'] = df_tmp['SCENARIO_ID'].astype(int)\n",
    "\n",
    "    # Merge demand for each node and scenario\n",
    "    df_demand = df_scenarios.loc[:, ('demand')].T\n",
    "    df_demand.index = df_demand.index.astype(int)\n",
    "    df_demand = df_demand.reset_index().melt(id_vars=['NODE_ID']).rename(columns={'value': 'demand'})\n",
    "    df_demand['closest_centroid'] = df_demand['closest_centroid'].astype(int)\n",
    "    df_tmp = pd.merge(df_tmp, df_demand, left_on=['SCENARIO_ID', 'NODE_ID'], right_on=['closest_centroid', 'NODE_ID'])\n",
    "\n",
    "    # Merge duration information for each scenario\n",
    "    df_duration = df_scenarios.loc[:, ('hours', 'duration')].to_frame()\n",
    "    df_duration.columns = df_duration.columns.droplevel()\n",
    "    df_tmp = pd.merge(df_tmp, df_duration, left_on='SCENARIO_ID', right_index=True)\n",
    "\n",
    "    # Compute total revenue and total energy demand\n",
    "    total_revenue = df_tmp.apply(lambda x: x['Value'] * x['demand'] * x['duration'], axis=1).sum()\n",
    "    total_demand = df_tmp.apply(lambda x: x['demand'] * x['duration'], axis=1).sum()\n",
    "\n",
    "    # Find average price (national)\n",
    "    average_price = total_revenue / total_demand\n",
    "    \n",
    "    return average_price\n",
    "\n",
    "dcopf_average_price = get_dcopf_average_price(df_dcopf, 'POWER_BALANCE')\n",
    "mppdc_average_price = df_mppdc['AVERAGE_PRICE'].unique()[0]\n",
    "\n",
    "# Save BAU average price - useful when constructing plots\n",
    "with open(os.path.join(output_dir, 'mppdc_bau_average_price.pickle'), 'wb') as f:\n",
    "    pickle.dump(mppdc_average_price, f)\n",
    "\n",
    "print('Absolute difference between DCOPF and MPPDC average prices: {0} [$/MWh]'.format(abs(dcopf_average_price - mppdc_average_price)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare nodal prices between DCOPF and MPPDC models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum difference between nodal prices over all nodes and scenarios: 4.738542003224211e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.738542003224211e-08"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_nodal_prices(df_dcopf, df_mppdc):\n",
    "    \"\"\"Find max absolute difference in nodal prices between DCOPF and MPPDC models\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_dcopf : pandas DataFrame\n",
    "        Results from DCOPF model\n",
    "    \n",
    "    df_mppdc : pandas DataFrame\n",
    "        Results from MPPDC model\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    max_price_difference : float\n",
    "        Maximum difference between nodal prices for DCOPF and MPPDC models\n",
    "        over all nodes and scenarios    \n",
    "    \"\"\"\n",
    "\n",
    "    # DCOPF model\n",
    "    # -----------\n",
    "    df_tmp_1 = df_dcopf.reset_index().copy()\n",
    "\n",
    "    # Filter price records\n",
    "    df_tmp_1 = df_tmp_1[df_tmp_1['index'].str.contains(r'\\.POWER_BALANCE\\[')]\n",
    "\n",
    "    # Extract values\n",
    "    df_tmp_1['Value'] = df_tmp_1.apply(lambda x: x['Constraint']['Dual'], axis=1)\n",
    "\n",
    "    # Extract node and scenario IDs\n",
    "    df_tmp_1['NODE_ID'] = df_tmp_1['index'].str.extract(r'\\.POWER_BALANCE\\[(\\d+)\\]').astype(int)\n",
    "    df_tmp_1['SCENARIO_ID'] = df_tmp_1['SCENARIO_ID'].astype(int)\n",
    "\n",
    "    # Prices at each node for each scenario\n",
    "    df_dcopf_prices = df_tmp_1.set_index(['SCENARIO_ID', 'NODE_ID'])['Value']\n",
    "\n",
    "\n",
    "    # MPPDC model\n",
    "    # -----------\n",
    "    df_tmp_2 = df_mppdc.reset_index().copy()\n",
    "\n",
    "    # Filter price records\n",
    "    df_tmp_2 = df_tmp_2[df_tmp_2['index'].str.contains(r'\\.lambda_var\\[')]\n",
    "\n",
    "    # Extract values\n",
    "    df_tmp_2['Value'] = df_tmp_2.apply(lambda x: x['Variable']['Value'], axis=1)\n",
    "\n",
    "    # Extract node and scenario IDs\n",
    "    df_tmp_2['NODE_ID'] = df_tmp_2['index'].str.extract(r'\\.lambda_var\\[(\\d+)\\]').astype(int)\n",
    "    df_tmp_2['SCENARIO_ID'] = df_tmp_2['index'].str.extract(r'LL_DUAL\\[(\\d+)\\]').astype(int)\n",
    "\n",
    "    # Prices at each node for each scenario\n",
    "    df_mppdc_prices = df_tmp_2.set_index(['SCENARIO_ID', 'NODE_ID'])['Value']\n",
    "\n",
    "    # Compute difference between models\n",
    "    # ---------------------------------\n",
    "    max_price_difference = df_dcopf_prices.subtract(df_mppdc_prices).abs().max()\n",
    "    print('Maximum difference between nodal prices over all nodes and scenarios: {0}'.format(max_price_difference))\n",
    "\n",
    "    return max_price_difference\n",
    "\n",
    "# Find max nodal price difference between DCOPF and MPPDC models\n",
    "compare_nodal_prices(df_dcopf=df_dcopf, df_mppdc=df_mppdc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close correspondence of price and output results between MPPDC and DCOPF representations suggests that the MPPDC has been formulated correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise data for plotting\n",
    "Using collated model results data, find:\n",
    "1. emissions intensity baselines that target average wholesale prices for different permit price scenarios;\n",
    "2. scheme revenue that corresponds with price targeting baselines and permit price scenarios;\n",
    "3. average regional and national prices under a REP scheme with different average wholesale price targets;\n",
    "4. average regional and national prices under a carbon tax.\n",
    "\n",
    "\n",
    "### Price targeting baselines for different average price targets and fixed permit prices\n",
    "Collated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price targeting baseline results - unprocessed\n",
    "df_price_targeting_baseline = collate_data('MPPDC-FIND_PRICE_TARGETING_BASELINE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baselines for different average price targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price targeting baseline as a function of permit price\n",
    "df_baseline_vs_permit_price = df_price_targeting_baseline.groupby(['FIXED_TAU', 'TARGET_PRICE_BAU_MULTIPLE']).apply(lambda x: x.loc['phi', 'Variable']['Value']).unstack()\n",
    "\n",
    "with open(os.path.join(output_dir, 'df_baseline_vs_permit_price.pickle'), 'wb') as f:\n",
    "    pickle.dump(df_baseline_vs_permit_price, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheme revenue for different average price targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_revenue(group):\n",
    "    \"Get scheme revenue for each permit price - price target scenario\"\n",
    "    \n",
    "    # Get baseline for each group\n",
    "    baseline = group.loc['phi', 'Variable']['Value']\n",
    "    \n",
    "    # Filter power output records\n",
    "    group = group[group.index.str.contains(r'LL_PRIM\\[\\d+\\].P\\[')].copy()\n",
    "    \n",
    "    # Extract DUID - used to merge generator information\n",
    "    group['DUID'] = group.apply(lambda x: re.findall(r'P\\[(.+)\\]', x.name)[0], axis=1)\n",
    "    \n",
    "    # Extract scenario ID - used to get duration of scenario\n",
    "    group['SCENARIO_ID'] = group.apply(lambda x: re.findall(r'LL_PRIM\\[(\\d+)\\]', x.name)[0], axis=1)\n",
    "\n",
    "    # Duration of each scenario [hours]\n",
    "    scenario_id = int(group['SCENARIO_ID'].unique()[0])\n",
    "    scenario_duration = df_scenarios.loc[scenario_id, ('hours', 'duration')]\n",
    "    \n",
    "    # Generator power output\n",
    "    group['Value'] = group.apply(lambda x: x['Variable']['Value'], axis=1)\n",
    "    \n",
    "    # Merge SRMCs and emissions intensities\n",
    "    group = pd.merge(group, df_g[['SRMC_2016-17', 'EMISSIONS']], how='left', left_on='DUID', right_index=True)\n",
    "    \n",
    "    # Compute revenue SUM ((emissions_intensity - baseline) * power_output * duration)\n",
    "    revenue = group['EMISSIONS'].subtract(baseline).mul(group['Value']).mul(group['FIXED_TAU']).mul(scenario_duration).sum() / 8760\n",
    "    \n",
    "    return revenue\n",
    "\n",
    "# Get revenue for each permit price and wholesale average price target scenario\n",
    "df_baseline_vs_revenue = df_price_targeting_baseline.groupby(['FIXED_TAU', 'TARGET_PRICE_BAU_MULTIPLE']).apply(get_revenue).unstack()\n",
    "\n",
    "with open(os.path.join(output_dir, 'df_baseline_vs_revenue.pickle'), 'wb') as f:\n",
    "    pickle.dump(df_baseline_vs_revenue, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regional and national average wholesale electricity prices for different average price targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_prices(group):\n",
    "    \"Compute regional and national average wholesale prices\"\n",
    "    \n",
    "    # Filter price records\n",
    "    group = group[group.index.str.contains(r'\\.lambda_var\\[')].reset_index().copy()\n",
    "    \n",
    "    # Extract nodal prices\n",
    "    group['Value'] = group.apply(lambda x: x['Variable']['Value'], axis=1)\n",
    "\n",
    "    # Extract node and scenario IDs\n",
    "    group['NODE_ID'] = group['index'].str.extract(r'lambda_var\\[(\\d+)\\]')[0].astype(int)\n",
    "    group['SCENARIO_ID'] = group['index'].str.extract(r'LL_DUAL\\[(\\d+)\\]')[0].astype(int)\n",
    "\n",
    "    # Scenario Demand\n",
    "    df_demand = df_scenarios.loc[:, ('demand')].reset_index().melt(id_vars=['closest_centroid'])\n",
    "    df_demand['NODE_ID'] = df_demand['NODE_ID'].astype(int)\n",
    "    df_demand = df_demand.rename(columns={'value': 'demand'})\n",
    "    group = pd.merge(group, df_demand, how='left', left_on=['NODE_ID', 'SCENARIO_ID'], right_on=['NODE_ID', 'closest_centroid'])\n",
    "\n",
    "    # Scenario duration\n",
    "    df_duration = df_scenarios.loc[:, ('hours', 'duration')].to_frame()\n",
    "    df_duration.columns = df_duration.columns.droplevel()\n",
    "    group = pd.merge(group, df_duration, how='left', left_on='SCENARIO_ID', right_index=True)\n",
    "\n",
    "    # NEM regions\n",
    "    group = pd.merge(group, df_n[['NEM_REGION']], how='left', left_on='NODE_ID', right_index=True)\n",
    "\n",
    "    # Compute node energy demand [MWh]\n",
    "    group['total_demand'] = group['demand'].mul(group['duration'])\n",
    "\n",
    "    # Compute node revenue [$]\n",
    "    group['total_revenue'] = group['total_demand'].mul(group['Value'])\n",
    "\n",
    "    # National average price\n",
    "    national_average_price = group['total_revenue'].sum() / group['total_demand'].sum()\n",
    "\n",
    "    # Regional averages\n",
    "    df_average_prices = group.groupby('NEM_REGION')[['total_demand', 'total_revenue']].sum().apply(lambda x: x['total_revenue'] / x['total_demand'], axis=1)\n",
    "    df_average_prices.loc['NATIONAL'] = national_average_price\n",
    "\n",
    "    return df_average_prices\n",
    "\n",
    "# REP Scheme average prices\n",
    "df_rep_average_prices = df_price_targeting_baseline.groupby(['FIXED_TAU', 'TARGET_PRICE_BAU_MULTIPLE']).apply(get_average_prices)\n",
    "\n",
    "with open(os.path.join(output_dir, 'df_rep_average_prices.pickle'), 'wb') as f:\n",
    "    pickle.dump(df_rep_average_prices, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average prices under a carbon tax\n",
    "Collate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carbon tax scenario results\n",
    "df_carbon_tax = collate_data('MPPDC-FIXED_PARAMETERS-BASELINE_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average prices under carbon tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average regional and national prices under a carbon tax\n",
    "df_carbon_tax_average_prices = df_carbon_tax.groupby(['FIXED_TAU', 'TARGET_PRICE_BAU_MULTIPLE']).apply(get_average_prices)\n",
    "\n",
    "with open(os.path.join(output_dir, 'df_carbon_tax_average_prices.pickle'), 'wb') as f:\n",
    "    pickle.dump(df_carbon_tax_average_prices, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average system emissions intensity for different permit prices\n",
    "Average system emissions intensity is invariant to the emissions intensity baseline (the emissions intensity baseline doesn't affect the relative costs of generators). Therefore the emissions outcomes will be the same for the REP and carbon tax scenarios when permit prices are fixed.\n",
    "\n",
    "**Note: this equivalence in outcomes assumes that demand is perfectly inelastic.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_emissions_intensity(group):\n",
    "    \"Get average emissions intensity for each permit price scenario\"\n",
    "    \n",
    "    # Reset index\n",
    "    df_tmp = group.reset_index()\n",
    "    df_tmp = df_tmp[df_tmp['index'].str.contains(r'\\.P\\[')].copy()\n",
    "    \n",
    "    # Extract generator IDs\n",
    "    df_tmp['DUID'] = df_tmp['index'].str.extract(r'\\.P\\[(.+)\\]')\n",
    "    \n",
    "    # Extract and format scenario IDs\n",
    "    df_tmp['SCENARIO_ID'] = df_tmp['index'].str.extract(r'LL_PRIM\\[(.+)\\]\\.')\n",
    "    df_tmp['SCENARIO_ID'] = df_tmp['SCENARIO_ID'].astype(int)\n",
    "    \n",
    "    # Extract power output values\n",
    "    df_tmp['value'] = df_tmp.apply(lambda x: x['Variable']['Value'], axis=1)\n",
    "\n",
    "    # Duration of each scenario\n",
    "    df_duration = df_scenarios.loc[:, ('hours', 'duration')].to_frame()\n",
    "    df_duration.columns = df_duration.columns.droplevel(0)\n",
    "\n",
    "    # Merge scenario duration information\n",
    "    df_tmp = pd.merge(df_tmp, df_duration, how='left', left_on='SCENARIO_ID', right_index=True)\n",
    "\n",
    "    # Merge emissions intensity information\n",
    "    df_tmp = pd.merge(df_tmp, df_g[['EMISSIONS']], how='left', left_on='DUID', right_index=True)\n",
    "\n",
    "    # Total emissions [tCO2]\n",
    "    total_emissions = df_tmp['value'].mul(df_tmp['duration']).mul(df_tmp['EMISSIONS']).sum()\n",
    "    \n",
    "    # Total demand [MWh]\n",
    "    total_demand = df_scenarios.loc[:, ('demand')].sum(axis=1).mul(df_duration['duration']).sum()    \n",
    "\n",
    "    # Average emissions intensity\n",
    "    average_emissions_intensity = total_emissions / total_demand\n",
    "    \n",
    "    return average_emissions_intensity\n",
    "\n",
    "# Average emissions intensity for different permit price scenarios\n",
    "df_average_emissions_intensities = df_carbon_tax.groupby('FIXED_TAU').apply(get_average_emissions_intensity)\n",
    "\n",
    "with open(os.path.join(output_dir, 'df_average_emissions_intensities.pickle'), 'wb') as f:\n",
    "    pickle.dump(df_average_emissions_intensities, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (opti)",
   "language": "python",
   "name": "opti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
