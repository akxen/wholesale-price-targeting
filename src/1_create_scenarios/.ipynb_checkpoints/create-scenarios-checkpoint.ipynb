{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario Construction\n",
    "Demand and dispatch data are obtained from the Australian Energy Market Operator's (AEMO's) Market Management System Database Model (MMSDM) [1], and a k-means clustering algorithm is implemented using the method outlined in [2] to create a reduced set of representative operating scenarios. The dataset described in [3,4] is used to identify specific generators, and assign historic power injections or withdrawals to individual nodes.\n",
    "\n",
    "In this analysis data for 2017 is considered, with demand and dispatch time series re-sampled to 30min intervals, corresponding to the length of a trading interval within Australia's National Electricity Market (NEM). Using these data a reduced set of 48 operating conditions are constructed. These operating scenarios are comprised on demand, intermittent renewable injections, and fixed power injections from hydro generators.\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialise random number generator\n",
    "random.seed(1)\n",
    "\n",
    "# Used to slice pandas DataFrames\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contains network information and generator parameters\n",
    "data_dir = os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, 'data')\n",
    "\n",
    "# MMSDM historic demand and dispatch signals\n",
    "archive_dir = r'D:\\nemweb\\Reports\\Data_Archive\\MMSDM\\zipped'\n",
    "\n",
    "# Location for output files\n",
    "output_dir = os.path.join(os.path.curdir, 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import generator and network information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator information\n",
    "df_g = pd.read_csv(os.path.join(data_dir, 'generators.csv'), index_col='DUID', dtype={'NODE': int})\n",
    "\n",
    "# Network node information\n",
    "df_n = pd.read_csv(os.path.join(data_dir, 'network_nodes.csv'), index_col='NODE_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data\n",
    "Functions used to extract data from MMSDM tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dispatch_unit_scada(file):\n",
    "    \"\"\"Extract generator dispatch data\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    file : bytes IO object\n",
    "        Zipped CSV file of given MMSDM table\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas DataFrame\n",
    "        MMSDM table in formatted pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Columns to extract    \n",
    "    cols = ['DUID', 'SCADAVALUE', 'SETTLEMENTDATE']\n",
    "\n",
    "    # Read in data\n",
    "    df = pd.read_csv(file, usecols=cols, parse_dates=['SETTLEMENTDATE'], skiprows=1)\n",
    "\n",
    "    # Drop rows without DUIDs, apply pivot\n",
    "    df = df.dropna(subset=['DUID']).pivot(index='SETTLEMENTDATE', columns='DUID', values='SCADAVALUE')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def tradingregionsum(file):\n",
    "    \"\"\"Extract half-hourly load data for each NEM region\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    file : bytes IO object\n",
    "        Zipped CSV file of given MMSDM table\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas DataFrame\n",
    "        MMSDM table in formatted pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Columns to extract    \n",
    "    cols = ['REGIONID', 'TOTALDEMAND', 'SETTLEMENTDATE']\n",
    "\n",
    "    # Read in data\n",
    "    df = pd.read_csv(file, usecols=cols, parse_dates=['SETTLEMENTDATE'], skiprows=1)\n",
    "\n",
    "    # Drop rows without DUIDs, apply pivot\n",
    "    df = df.dropna(subset=['REGIONID']).pivot(index='SETTLEMENTDATE', columns='REGIONID', values='TOTALDEMAND')\n",
    "\n",
    "    return df  \n",
    "\n",
    "\n",
    "def get_data(archive_path, table_name, extractor_function):\n",
    "    \"\"\"Open CSV archive and extract data from zipped file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    archive_path : str\n",
    "        Path to MMSDM archive containing data for given year\n",
    "    \n",
    "    table_name : str\n",
    "        Name of table in MMSDM archive from which data is to be extracted\n",
    "        \n",
    "    extractor_function : func\n",
    "        Function that takes a bytes object of the unzipped table and returns a formatted DataFrame\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas DataFrame\n",
    "        Formatted DataFrame of the desired MMSDM table    \n",
    "    \"\"\"\n",
    "\n",
    "    # Open MMSDM archive for a given year\n",
    "    with zipfile.ZipFile(archive_path) as myzip:\n",
    "        \n",
    "        # All files of a particular type in archive (e.g. dispatch, quantity bids, price bands, load)\n",
    "        zip_names = [f for f in myzip.filelist if (table_name in f.filename) and ('.zip' in f.filename)]\n",
    "\n",
    "        # Check that only one zip file is returned, else raise exception\n",
    "        if len(zip_names) != 1:\n",
    "            raise Exception('Encounted {0} files in archive, should only encounter 1'.format(len(zip_names)))\n",
    "\n",
    "        # Get name of csv in zipped folder\n",
    "        csv_name = zip_names[0].filename.replace('.zip', '.CSV').split('/')[-1]\n",
    "\n",
    "        # Convert zip files to BytesIO object\n",
    "        zip_data = BytesIO(myzip.read(zip_names[0]))\n",
    "\n",
    "        # Open inner zipfile and extract data using supplied function\n",
    "        with zipfile.ZipFile(zip_data) as z:\n",
    "            with z.open(csv_name) as f:\n",
    "                df = extractor_function(f)      \n",
    "    return df\n",
    "\n",
    "# Historic demand and dispatch data\n",
    "demand = []\n",
    "dispatch = []\n",
    "\n",
    "for i in range(1, 13):\n",
    "    # Archive name and path from which data will be extracted\n",
    "    archive_name = 'MMSDM_2017_{0:02}.zip'.format(i)\n",
    "    archive_path = os.path.join(archive_dir, archive_name)\n",
    "\n",
    "    # Extract data\n",
    "    dispatch.append(get_data(archive_path, 'DISPATCH_UNIT_SCADA', dispatch_unit_scada))\n",
    "    demand.append(get_data(archive_path, 'TRADINGREGIONSUM', tradingregionsum))\n",
    "\n",
    "# Concatenate data from individual months into single DataFrames for load and dispatch\n",
    "df_demand = pd.concat(demand, sort=True) # Demand\n",
    "df_dispatch = pd.concat(dispatch, sort=True) # Dispatch\n",
    "\n",
    "# Fill missing values\n",
    "df_demand = df_demand.fillna(0)\n",
    "    \n",
    "# Resample to get average power output over 30min trading interval (instead of 5min) dispatch intervals\n",
    "df_dispatch = df_dispatch.resample('30min', label='right', closed='right').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-index and format data:\n",
    "\n",
    "1. identify intermittent generators (wind and solar);\n",
    "2. identify hydro generators;\n",
    "3. compute nodal demand;\n",
    "4. concatenate demand, hydro dispatch, and intermittent renewables dispatch into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Intermittent generators\n",
    "mask_intermittent = df_g['FUEL_CAT'].isin(['Wind', 'Solar'])\n",
    "df_g[mask_intermittent]\n",
    "\n",
    "# Intermittent dispatch at each node\n",
    "df_intermittent = (df_dispatch\n",
    "                   .T\n",
    "                   .join(df_g.loc[mask_intermittent, 'NODE'], how='left')\n",
    "                   .groupby('NODE').sum()\n",
    "                   .reindex(df_n.index, fill_value=0))\n",
    "df_intermittent['level'] = 'intermittent'\n",
    "\n",
    "# Hydro generators\n",
    "mask_hydro = df_g['FUEL_CAT'].isin(['Hydro'])\n",
    "df_g[mask_hydro]\n",
    "\n",
    "# Hydro dispatch at each node\n",
    "df_hydro = (df_dispatch\n",
    "            .T\n",
    "            .join(df_g.loc[mask_hydro, 'NODE'], how='left')\n",
    "            .groupby('NODE').sum()\n",
    "            .reindex(df_n.index, fill_value=0))\n",
    "df_hydro['level'] = 'hydro'\n",
    "\n",
    "# Demand at each node\n",
    "def node_demand(row):\n",
    "    return df_demand[row['NEM_REGION']] * row['PROP_REG_D']\n",
    "df_node_demand = df_n.apply(node_demand, axis=1)\n",
    "df_node_demand['level'] = 'demand'\n",
    "\n",
    "# Concatenate intermittent, hydro, and demand series, add level to index\n",
    "df_o = (pd.concat([df_node_demand, df_intermittent, df_hydro])\n",
    "        .set_index('level', append=True)\n",
    "        .reorder_levels(['level', 'NODE_ID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbours\n",
    "Construct clustering algorithm to transform the set of trading intervals into a reduced set of representative operating scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_scenarios(df, k=1, max_iterations=100, stopping_tolerance=0):\n",
    "    \"\"\"Create representative demand and fixed power injection operating scenarios\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        Input DataFrame from which representative operating conditions \n",
    "        should be constructed\n",
    "\n",
    "    k : int\n",
    "        Number of clusters\n",
    "\n",
    "    max_iterations : int\n",
    "        Max number of iterations used to find centroid\n",
    "\n",
    "    stopping_tolerance : float\n",
    "        Max total difference between successive centroid iteration DataFrames\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_clustered : pandas DataFrame\n",
    "        Operating scenario centroids and their associated duration.\n",
    "\n",
    "    centroid_history : dict\n",
    "        Dictionary where keys are the iteration number and values are a DataFrame describing\n",
    "        the allocation of operating conditions to centroids, and the distance between these\n",
    "        values. \n",
    "    \"\"\"\n",
    "\n",
    "    # Random time periods used to initialise centroids\n",
    "    random_periods = random.sample(list(df.columns), k)\n",
    "    df_centroids = df[random_periods]\n",
    "\n",
    "    # Rename centroid DataFrame columns and keep track of initial labels\n",
    "    timestamp_map = {timestamp: timestamp_key + 1 for timestamp_key, timestamp in enumerate(df_centroids.columns)}\n",
    "    df_centroids = df_centroids.rename(columns=timestamp_map)\n",
    "\n",
    "    def compute_distance(col):\n",
    "        \"\"\"Compute distance between each data associated with each trading interval, col, and all centroids.\n",
    "        Return closest centroid.\n",
    "\n",
    "        Params\n",
    "        ------\n",
    "        col : pandas Series\n",
    "            Operating condition for trading interval\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        closest_centroid_ID : pandas Series\n",
    "            Series with ID of closest centroid and the distance to that centroid\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialise minimum distance between data constituting a trading interval and all centroids to an \n",
    "        # arbitrarily large number\n",
    "        min_distance = 9e9\n",
    "        \n",
    "        # Initially no centroid is defined as being closest to the given trading interval\n",
    "        closest_centroid = None\n",
    "\n",
    "        # Compute Euclidean (2-norm) distance between the data describing a trading interval, col, and\n",
    "        # all centroids. Identify the closest centroid for the given trading interval.\n",
    "        for centroid in df_centroids.columns:\n",
    "            distance = math.sqrt(sum((df_centroids[centroid] - col) ** 2))\n",
    "\n",
    "            # If present value less than minimum distance, update minimum distance and record centroid\n",
    "            if distance <= min_distance:\n",
    "                min_distance = distance\n",
    "                closest_centroid = centroid\n",
    "\n",
    "        # Return ID of closest centroid\n",
    "        closest_centroid_ID = pd.Series(data={'closest_centroid': closest_centroid, 'distance': min_distance})\n",
    "\n",
    "        return closest_centroid_ID\n",
    "\n",
    "\n",
    "    def update_centroids(row):\n",
    "        \"Update centroids by taking element-wise mean value of all vectors in cluster\"\n",
    "        \n",
    "        return df[row['SETTLEMENTDATE']].mean(axis=1)\n",
    "\n",
    "    # History of computed centroids\n",
    "    centroid_history = dict()\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # Get closest centroids for each trading interval and save result to dictionary\n",
    "        df_closest_centroids = df.apply(compute_distance)\n",
    "        centroid_history[i] = df_closest_centroids\n",
    "\n",
    "        # Timestamps belonging to each cluster\n",
    "        clustered_timestamps = (df_closest_centroids.loc['closest_centroid']\n",
    "                                .to_frame()\n",
    "                                .reset_index()\n",
    "                                .groupby('closest_centroid').agg(lambda x: list(x)))\n",
    "\n",
    "        # Update centroids by computing average nodal values across series in each cluster\n",
    "        df_centroids = clustered_timestamps.apply(update_centroids, axis=1).T\n",
    "\n",
    "        # If first iteration, set total absolute distance to arbitrarily large number\n",
    "        if i == 0:\n",
    "            total_absolute_distance = 1e7\n",
    "\n",
    "            # Lagged DataFrame in next iteration = DataFrame in current iteration\n",
    "            df_centroids_lag = df_centroids\n",
    "        \n",
    "        else:\n",
    "            # Element-wise absolute difference between current and previous centroid DataFrames\n",
    "            df_centroids_update_distance = abs(df_centroids - df_centroids_lag)\n",
    "\n",
    "            # Max total element-wise distance\n",
    "            total_absolute_distance = df_centroids_update_distance.sum().sum()\n",
    "\n",
    "            # Stopping condition\n",
    "            if total_absolute_distance <= stopping_tolerance:\n",
    "                print('Iteration number: {0} - Total absolute distance: {1}. Stopping criterion satisfied. Exiting loop.'.format(i+1, total_absolute_distance))\n",
    "                break\n",
    "            else:\n",
    "                # Continue loop \n",
    "                df_centroids_lag = df_centroids   \n",
    "\n",
    "        print('Iteration number: {0} - Difference between iterations: {1}'.format(i+1, total_absolute_distance))\n",
    "        \n",
    "        # Raise warning if loop terminates before stopping condition met\n",
    "        if i == (max_iterations - 1):\n",
    "            print('Max iteration limit exceeded before stopping tolerance satisfied.')\n",
    "\n",
    "\n",
    "    # Get duration for each scenario\n",
    "    # ------------------------------\n",
    "    # Length of each trading interval (hours)\n",
    "    interval_length = 0.5\n",
    "\n",
    "    # Total number of hours for each scenario\n",
    "    scenario_hours = (clustered_timestamps.apply(lambda x: len(x['SETTLEMENTDATE']), axis=1)\n",
    "                      .to_frame().T\n",
    "                      .mul(interval_length))\n",
    "\n",
    "    # Renaming and setting duration index values\n",
    "    scenario_hours = scenario_hours.rename(index={0: 'hours'})\n",
    "    scenario_hours['level'] = 'duration'\n",
    "    scenario_hours.set_index('level', append=True, inplace=True)\n",
    "\n",
    "    # Final DataFrame with clustered values\n",
    "    df_clustered = pd.concat([df_centroids, scenario_hours])\n",
    "\n",
    "    # Convert column labels to type int\n",
    "    df_clustered.columns = df_clustered.columns.astype(int)\n",
    "\n",
    "    return df_clustered, centroid_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create operating scenarios\n",
    "Create operating scenarios for different numbers of clusters and save to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 1 - Difference between iterations: 10000000.0\n",
      "Iteration number: 2 - Difference between iterations: 44872.23773754929\n",
      "Iteration number: 3 - Difference between iterations: 28923.76265257388\n",
      "Iteration number: 4 - Difference between iterations: 20897.04798111056\n",
      "Iteration number: 5 - Difference between iterations: 15480.767642873347\n",
      "Iteration number: 6 - Difference between iterations: 12484.768446179873\n",
      "Iteration number: 7 - Difference between iterations: 9998.374403476835\n",
      "Iteration number: 8 - Difference between iterations: 9016.496824554233\n",
      "Iteration number: 9 - Difference between iterations: 8692.182458056617\n",
      "Iteration number: 10 - Difference between iterations: 7989.27306254635\n",
      "Iteration number: 11 - Difference between iterations: 7771.20160855181\n",
      "Iteration number: 12 - Difference between iterations: 7535.567204322281\n",
      "Iteration number: 13 - Difference between iterations: 6681.957822619\n",
      "Iteration number: 14 - Difference between iterations: 6063.21184936421\n",
      "Iteration number: 15 - Difference between iterations: 5215.424862353642\n",
      "Iteration number: 16 - Difference between iterations: 5562.561987406691\n",
      "Iteration number: 17 - Difference between iterations: 4636.872155246882\n",
      "Iteration number: 18 - Difference between iterations: 4151.283815569257\n",
      "Iteration number: 19 - Difference between iterations: 3367.486504039685\n",
      "Iteration number: 20 - Difference between iterations: 3459.5770540746457\n",
      "Iteration number: 21 - Difference between iterations: 3245.760539723998\n",
      "Iteration number: 22 - Difference between iterations: 3143.0057158004997\n",
      "Iteration number: 23 - Difference between iterations: 2853.8722473869843\n",
      "Iteration number: 24 - Difference between iterations: 2948.263029603487\n",
      "Iteration number: 25 - Difference between iterations: 3178.291647564428\n",
      "Iteration number: 26 - Difference between iterations: 3090.058478165758\n",
      "Iteration number: 27 - Difference between iterations: 3132.6272932979864\n",
      "Iteration number: 28 - Difference between iterations: 2883.076964496857\n",
      "Iteration number: 29 - Difference between iterations: 2795.3964481259122\n",
      "Iteration number: 30 - Difference between iterations: 2390.457020044656\n",
      "Iteration number: 31 - Difference between iterations: 2221.2255906327273\n",
      "Iteration number: 32 - Difference between iterations: 1759.969622790314\n",
      "Iteration number: 33 - Difference between iterations: 1770.7633197404978\n",
      "Iteration number: 34 - Difference between iterations: 1680.3575206390276\n",
      "Iteration number: 35 - Difference between iterations: 1783.2077282971384\n",
      "Iteration number: 36 - Difference between iterations: 1491.049419327789\n",
      "Iteration number: 37 - Difference between iterations: 1447.7904656385074\n",
      "Iteration number: 38 - Difference between iterations: 1256.9267791535688\n",
      "Iteration number: 39 - Difference between iterations: 940.6514017071211\n",
      "Iteration number: 40 - Difference between iterations: 884.3361346740842\n",
      "Iteration number: 41 - Difference between iterations: 852.7413183370817\n",
      "Iteration number: 42 - Difference between iterations: 1067.5105096582222\n",
      "Iteration number: 43 - Difference between iterations: 1102.0147548261473\n",
      "Iteration number: 44 - Difference between iterations: 945.0963234945507\n",
      "Iteration number: 45 - Difference between iterations: 970.5755968229757\n",
      "Iteration number: 46 - Difference between iterations: 609.280009600923\n",
      "Iteration number: 47 - Difference between iterations: 387.9318250670119\n",
      "Iteration number: 48 - Difference between iterations: 271.39762944419186\n",
      "Iteration number: 49 - Difference between iterations: 216.32451261327768\n",
      "Iteration number: 50 - Difference between iterations: 190.70642500194035\n",
      "Iteration number: 51 - Difference between iterations: 168.16605745847238\n",
      "Iteration number: 52 - Difference between iterations: 78.9477029701386\n",
      "Iteration number: 53 - Difference between iterations: 66.12790849852881\n",
      "Iteration number: 54 - Difference between iterations: 32.896363159693706\n",
      "Iteration number: 55 - Difference between iterations: 38.53012359293304\n",
      "Iteration number: 56 - Total absolute distance: 0.0. Stopping criterion satisfied. Exiting loop.\n"
     ]
    }
   ],
   "source": [
    "# Create operating scenarios for different numbers of clusters\n",
    "for k in [48, 100]:\n",
    "    # Create operating scenarios\n",
    "    df_clustered, _ = create_scenarios(df=df_o, k=k, max_iterations=int(9e9), stopping_tolerance=0)\n",
    "    \n",
    "    # Save scenarios\n",
    "    with open(os.path.join(output_dir, '{0}_scenarios.pickle'.format(k)), 'wb') as f:\n",
    "        pickle.dump(df_clustered, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] - Australian Energy Markets Operator. Data Archive (2018). at [http://www.nemweb.com.au/#mms-data-model:download](http://www.nemweb.com.au/#mms-data-model:download)\n",
    "\n",
    "[2] - Baringo L., Conejo, A. J., Correlated wind-power production and electric load scenarios for investment decisions. Applied Energy (2013).\n",
    "\n",
    "[3] - Xenophon A. K., Hill D. J., Geospatial modelling of Australia's National Electricity Market allowing backtesting against historic data.  Scientific Data (2018).\n",
    "\n",
    "[4] - Xenophon A. K., Hill D. J., Geospatial Modelling of Australia's National Electricity Market - Dataset (Version v1.3) [Data set]. Zenodo. [http://doi.org/10.5281/zenodo.1326942](http://doi.org/10.5281/zenodo.1326942)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opti env",
   "language": "python",
   "name": "opti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
